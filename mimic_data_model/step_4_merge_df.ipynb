{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge\n",
    "In this notebook, we will do the following tables.\n",
    "```\n",
    "0     PROCEDUREEVENTS_MV\n",
    "1                D_ITEMS\n",
    "2     MICROBIOLOGYEVENTS\n",
    "3              LABEVENTS\n",
    "4         INPUTEVENTS_CV\n",
    "5             ADMISSIONS\n",
    "6             D_LABITEMS\n",
    "7         DATETIMEEVENTS\n",
    "8          PRESCRIPTIONS\n",
    "9         PROCEDURES_ICD\n",
    "10           CHARTEVENTS\n",
    "11             TRANSFERS\n",
    "12         DIAGNOSES_ICD\n",
    "13              SERVICES\n",
    "14              DRGCODES\n",
    "15          OUTPUTEVENTS\n",
    "16              PATIENTS\n",
    "17       D_ICD_DIAGNOSES\n",
    "18              ICUSTAYS\n",
    "19        INPUTEVENTS_MV\n",
    "20      D_ICD_PROCEDURES\n",
    "```\n",
    "Some functions we need to implement are:\n",
    "1. functions to read in two csv files with selected column\n",
    "2. create a schema based on schema definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType, DoubleType\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/24 20:31:37 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "conf = SparkConf()  # create the configuration\n",
    "conf.setMaster(\"local\")\n",
    "# conf.set(\"spark.jars\", \"/Users/yixiangzhang/Desktop/postgresql-42.4.1.jar\")\n",
    "# conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark = SparkSession.builder\\\n",
    "                    .config(conf = conf)\\\n",
    "                    .appName('test').getOrCreate()\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.2.11:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1229c7290>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populate `Transfers` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mimic3</th>\n",
       "      <th>mimic4</th>\n",
       "      <th>column_mimic3</th>\n",
       "      <th>column_mimic4_candidate_1</th>\n",
       "      <th>candidate_1_scores</th>\n",
       "      <th>dtype_mimic3</th>\n",
       "      <th>mimic3_filepath</th>\n",
       "      <th>mimic4_filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>TRANSFERS</td>\n",
       "      <td>transfers.csv</td>\n",
       "      <td>subject_id</td>\n",
       "      <td>subject_id</td>\n",
       "      <td>100</td>\n",
       "      <td>int</td>\n",
       "      <td>../basic_filtered_data/mimic-iii-demo/TRANSFER...</td>\n",
       "      <td>../basic_filtered_data/mimic-iv-demo/2.2/hosp/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>TRANSFERS</td>\n",
       "      <td>transfers.csv</td>\n",
       "      <td>hadm_id</td>\n",
       "      <td>hadm_id</td>\n",
       "      <td>100</td>\n",
       "      <td>int</td>\n",
       "      <td>../basic_filtered_data/mimic-iii-demo/TRANSFER...</td>\n",
       "      <td>../basic_filtered_data/mimic-iv-demo/2.2/hosp/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>TRANSFERS</td>\n",
       "      <td>transfers.csv</td>\n",
       "      <td>eventtype</td>\n",
       "      <td>eventtype</td>\n",
       "      <td>100</td>\n",
       "      <td>string</td>\n",
       "      <td>../basic_filtered_data/mimic-iii-demo/TRANSFER...</td>\n",
       "      <td>../basic_filtered_data/mimic-iv-demo/2.2/hosp/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>TRANSFERS</td>\n",
       "      <td>transfers.csv</td>\n",
       "      <td>curr_careunit</td>\n",
       "      <td>careunit</td>\n",
       "      <td>90</td>\n",
       "      <td>string</td>\n",
       "      <td>../basic_filtered_data/mimic-iii-demo/TRANSFER...</td>\n",
       "      <td>../basic_filtered_data/mimic-iv-demo/2.2/hosp/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>TRANSFERS</td>\n",
       "      <td>transfers.csv</td>\n",
       "      <td>intime</td>\n",
       "      <td>intime</td>\n",
       "      <td>100</td>\n",
       "      <td>datetime</td>\n",
       "      <td>../basic_filtered_data/mimic-iii-demo/TRANSFER...</td>\n",
       "      <td>../basic_filtered_data/mimic-iv-demo/2.2/hosp/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>TRANSFERS</td>\n",
       "      <td>transfers.csv</td>\n",
       "      <td>outtime</td>\n",
       "      <td>outtime</td>\n",
       "      <td>100</td>\n",
       "      <td>datetime</td>\n",
       "      <td>../basic_filtered_data/mimic-iii-demo/TRANSFER...</td>\n",
       "      <td>../basic_filtered_data/mimic-iv-demo/2.2/hosp/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mimic3         mimic4  column_mimic3 column_mimic4_candidate_1   \n",
       "124  TRANSFERS  transfers.csv     subject_id                subject_id  \\\n",
       "125  TRANSFERS  transfers.csv        hadm_id                   hadm_id   \n",
       "126  TRANSFERS  transfers.csv      eventtype                 eventtype   \n",
       "127  TRANSFERS  transfers.csv  curr_careunit                  careunit   \n",
       "128  TRANSFERS  transfers.csv         intime                    intime   \n",
       "129  TRANSFERS  transfers.csv        outtime                   outtime   \n",
       "\n",
       "     candidate_1_scores dtype_mimic3   \n",
       "124                 100          int  \\\n",
       "125                 100          int   \n",
       "126                 100       string   \n",
       "127                  90       string   \n",
       "128                 100     datetime   \n",
       "129                 100     datetime   \n",
       "\n",
       "                                       mimic3_filepath   \n",
       "124  ../basic_filtered_data/mimic-iii-demo/TRANSFER...  \\\n",
       "125  ../basic_filtered_data/mimic-iii-demo/TRANSFER...   \n",
       "126  ../basic_filtered_data/mimic-iii-demo/TRANSFER...   \n",
       "127  ../basic_filtered_data/mimic-iii-demo/TRANSFER...   \n",
       "128  ../basic_filtered_data/mimic-iii-demo/TRANSFER...   \n",
       "129  ../basic_filtered_data/mimic-iii-demo/TRANSFER...   \n",
       "\n",
       "                                       mimic4_filepath  \n",
       "124  ../basic_filtered_data/mimic-iv-demo/2.2/hosp/...  \n",
       "125  ../basic_filtered_data/mimic-iv-demo/2.2/hosp/...  \n",
       "126  ../basic_filtered_data/mimic-iv-demo/2.2/hosp/...  \n",
       "127  ../basic_filtered_data/mimic-iv-demo/2.2/hosp/...  \n",
       "128  ../basic_filtered_data/mimic-iv-demo/2.2/hosp/...  \n",
       "129  ../basic_filtered_data/mimic-iv-demo/2.2/hosp/...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"./silver_data/TRANSFERS.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic3_path = df[\"mimic3_filepath\"].iloc[0]\n",
    "mimic3_selected = df[\"column_mimic3\"].to_list()\n",
    "mimic4_path = df[\"mimic4_filepath\"].iloc[0]\n",
    "mimic4_selected = df[\"column_mimic4_candidate_1\"].to_list()\n",
    "dtypes = df[\"dtype_mimic3\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/24 20:31:38 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Read compressed CSV\").config(\"spark.sql.inMemoryColumnarStorage.compressed\", \"true\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m3 = spark.read.csv(mimic3_path, \n",
    "                       header=True,\n",
    "                       inferSchema=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename two functions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(data,from_cols,to_cols):\n",
    "    \"\"\"map a dataframe with original columns to destination columns\n",
    "\n",
    "    :param _type_ data: pyspark dataframe\n",
    "    :param list from_cols: list of original name\n",
    "    :param list to_cols: list of destination name\n",
    "    :return _type_: renamed dataframe \n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(len(from_cols)):\n",
    "        # if columns names are different, then dump in mapper\n",
    "        if from_cols[i] != to_cols[i]:\n",
    "            data = data.withColumnRenamed(from_cols[i],to_cols[i])\n",
    "    \n",
    "    return data\n",
    "    \n",
    "\n",
    "df_m3 = rename_columns(data = df_m3,\n",
    "                        from_cols=mimic3_selected,\n",
    "                        to_cols=mimic4_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m3_selected = df_m3.select(mimic4_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+--------+-------------------+-------------------+\n",
      "|subject_id|hadm_id|eventtype|careunit|             intime|            outtime|\n",
      "+----------+-------+---------+--------+-------------------+-------------------+\n",
      "|     10006| 142345|    admit|    MICU|2164-10-23 21:10:15|2164-10-25 12:21:07|\n",
      "|     10006| 142345| transfer|    null|2164-10-25 12:21:07|2164-11-01 17:14:27|\n",
      "|     10006| 142345|discharge|    null|2164-11-01 17:14:27|               null|\n",
      "|     10011| 105331|    admit|    MICU|2126-08-14 22:34:00|2126-08-28 18:59:00|\n",
      "|     10011| 105331|discharge|    null|2126-08-28 18:59:00|               null|\n",
      "|     10013| 165520|    admit|    MICU|2125-10-04 23:38:00|2125-10-07 15:13:52|\n",
      "|     10013| 165520|discharge|    null|2125-10-07 15:13:52|               null|\n",
      "|     10017| 199207|    admit|    null|2149-05-26 17:21:09|2149-05-26 19:15:46|\n",
      "|     10017| 199207| transfer|    null|2149-05-26 19:15:46|2149-05-26 19:22:45|\n",
      "|     10017| 199207| transfer|    null|2149-05-26 19:22:45|2149-05-29 18:52:29|\n",
      "|     10017| 199207| transfer|     CCU|2149-05-29 18:52:29|2149-05-31 22:19:17|\n",
      "|     10017| 199207| transfer|    null|2149-05-31 22:19:17|2149-06-03 18:43:21|\n",
      "|     10017| 199207|discharge|    null|2149-06-03 18:43:21|               null|\n",
      "|     10019| 177759|    admit|    MICU|2163-05-14 20:43:56|2163-05-16 03:47:04|\n",
      "|     10019| 177759|discharge|    null|2163-05-16 03:47:04|               null|\n",
      "|     10026| 103770|    admit|    SICU|2195-05-17 07:40:18|2195-05-19 17:24:25|\n",
      "|     10026| 103770| transfer|    null|2195-05-19 17:24:25|2195-05-21 16:55:21|\n",
      "|     10026| 103770| transfer|    null|2195-05-21 16:55:21|2195-05-24 11:49:34|\n",
      "|     10026| 103770|discharge|    null|2195-05-24 11:49:34|               null|\n",
      "|     10027| 199395|    admit|    null|2190-07-13 03:16:35|2190-07-13 10:39:07|\n",
      "+----------+-------+---------+--------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_m3_selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappers_pandas_types_to_spark_types = {\n",
    "    \"int\": IntegerType(),\n",
    "    \"string\": StringType(),\n",
    "    \"datetime\": TimestampType(),\n",
    "    \"float\": DoubleType()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('subject_id', IntegerType(), True), StructField('hadm_id', IntegerType(), True), StructField('eventtype', StringType(), True), StructField('careunit', StringType(), True), StructField('intime', TimestampType(), True), StructField('outtime', TimestampType(), True)])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a mapper for datatypes\n",
    "def create_schema(pandas_types, mappers, column_names):\n",
    "    \"\"\"\n",
    "    map pandas data types such as int and float to pyspark dtypes such as .\n",
    "    \n",
    "    :param list pandas_types: pandas_types\n",
    "    :param dict mappers: a mapper\n",
    "    :param list column_names: column names you wish to put in the schema\n",
    "    :return pyspark.sql.types.StructType: _description_\n",
    "    \"\"\"\n",
    "    # map from pandas datatype to pyspark dtype\n",
    "    data_types = [mappers[type] for type in pandas_types]    \n",
    "    \n",
    "    # create fields\n",
    "    fields = [StructField(name, dataType) for name, dataType in zip(column_names, data_types)]\n",
    "    schema = StructType(fields)\n",
    "\n",
    "    return schema\n",
    "\n",
    "transfers_schema = create_schema(pandas_types =df[\"dtype_mimic3\"].to_list(),\n",
    "                                 mappers = mappers_pandas_types_to_spark_types,\n",
    "                                 column_names=mimic4_selected)\n",
    "\n",
    "transfers_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- subject_id: string (nullable = true)\n",
      " |-- hadm_id: string (nullable = true)\n",
      " |-- eventtype: string (nullable = true)\n",
      " |-- careunit: string (nullable = true)\n",
      " |-- intime: string (nullable = true)\n",
      " |-- outtime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_m3_selected.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_schema(df,schema):\n",
    "    \"\"\"\n",
    "\n",
    "    :param _type_ df: _description_\n",
    "    :param _type_ schema: _description_\n",
    "    :return _type_: _description_\n",
    "    \"\"\"\n",
    "    df_casted = df.select([df[field.name].cast(field.dataType) for field in schema.fields])\n",
    "    \n",
    "    return df_casted\n",
    "\n",
    "df_m3_casted = cast_schema(df = df_m3_selected,\n",
    "                           schema = transfers_schema\n",
    "                           )\n",
    "\n",
    "# df_m3_casted = df_m3_selected.select([df_m3_selected[field.name].cast(field.dataType) for field in transfers_schema.fields])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- subject_id: integer (nullable = true)\n",
      " |-- hadm_id: integer (nullable = true)\n",
      " |-- eventtype: string (nullable = true)\n",
      " |-- careunit: string (nullable = true)\n",
      " |-- intime: timestamp (nullable = true)\n",
      " |-- outtime: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_m3_casted.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the same thing for mimic4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m4 = spark.read.csv(mimic4_path, \n",
    "                       header=True,\n",
    "                       inferSchema=False)\n",
    "\n",
    "df_m4_selected = df_m4.select(mimic4_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- subject_id: string (nullable = true)\n",
      " |-- hadm_id: string (nullable = true)\n",
      " |-- eventtype: string (nullable = true)\n",
      " |-- careunit: string (nullable = true)\n",
      " |-- intime: string (nullable = true)\n",
      " |-- outtime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_m4_selected.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- subject_id: integer (nullable = true)\n",
      " |-- hadm_id: integer (nullable = true)\n",
      " |-- eventtype: string (nullable = true)\n",
      " |-- careunit: string (nullable = true)\n",
      " |-- intime: timestamp (nullable = true)\n",
      " |-- outtime: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_m4_casted = cast_schema(df = df_m4_selected,\n",
    "                           schema = transfers_schema\n",
    "                           )\n",
    "df_m4_casted.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge these two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+--------+-------------------+-------+\n",
      "|subject_id| hadm_id|eventtype|careunit|             intime|outtime|\n",
      "+----------+--------+---------+--------+-------------------+-------+\n",
      "|  10009049|22995465|discharge|    null|2174-05-31 14:21:47|   null|\n",
      "|  10025612|23403708|discharge|    null|2125-10-03 12:25:27|   null|\n",
      "|  10020786|23488445|discharge|    null|2189-06-13 17:25:44|   null|\n",
      "|  10014078|25809882|discharge|    null|2166-08-26 14:49:42|   null|\n",
      "|  10039831|26924951|discharge|    null|2116-01-02 14:35:02|   null|\n",
      "|  10029484|20764029|discharge|    null|2160-11-11 11:40:33|   null|\n",
      "|  10019568|28710730|discharge|    null|2120-02-02 15:40:32|   null|\n",
      "|  10020640|27984218|discharge|    null|2153-02-20 13:53:45|   null|\n",
      "|  10002495|24982426|discharge|    null|2141-05-29 17:42:52|   null|\n",
      "|  10007058|22954658|discharge|    null|2167-11-11 14:26:31|   null|\n",
      "|  10004422|21255400|discharge|    null|2111-01-25 15:34:47|   null|\n",
      "|  10037975|27617929|discharge|    null|2185-01-22 16:16:52|   null|\n",
      "|  10011398|27505812|discharge|    null|2146-12-19 13:37:17|   null|\n",
      "|  10009628|25926192|discharge|    null|2153-09-25 13:24:22|   null|\n",
      "|  10007928|20338077|discharge|    null|2129-04-11 17:45:53|   null|\n",
      "|  10023771|20044587|discharge|    null|2113-08-30 14:19:30|   null|\n",
      "|  10022017|22342963|discharge|    null|2189-09-16 15:34:08|   null|\n",
      "|  10006053|22942076|discharge|    null|2111-11-15 18:21:10|   null|\n",
      "|  10018845|21101111|discharge|    null|2184-10-11 17:36:21|   null|\n",
      "|  10031404|21606243|discharge|    null|2113-08-06 20:57:58|   null|\n",
      "+----------+--------+---------+--------+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transfer_merged = df_m4_casted.coalesce(1).union(df_m3_casted.coalesce(1))\n",
    "df_transfer_merged.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_db(data_frame, table_name, db_name, db_usrname, db_pssword,port):\n",
    "    data_frame.write.format('jdbc').options(\n",
    "              url=f'jdbc:postgresql://localhost:{port}/{db_name}',\n",
    "              driver='org.postgresql.Driver',\n",
    "              dbtable=table_name,\n",
    "              user=db_usrname,\n",
    "              password=db_pssword).\\\n",
    "              mode('overwrite') \\\n",
    "              .save()\n",
    "    \n",
    "    out_message = f\"Data frame {data_frame} has been appended to table {table_name} in the PostgreSQL database.\"\n",
    "    return out_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data frame DataFrame[subject_id: int, hadm_id: int, eventtype: string, careunit: string, intime: timestamp, outtime: timestamp] has been appended to table transfers in the PostgreSQL database.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_to_db(data_frame=df_transfer_merged,\n",
    "            table_name=\"transfers\",\n",
    "            db_name=\"mimic\",\n",
    "            db_usrname=\"mimic\",\n",
    "            db_pssword=\"mimic\",\n",
    "            port=6432)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
