{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType, DoubleType\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from util import spark_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "conf = SparkConf()  # create the configuration\n",
    "conf.setMaster(\"local\")\n",
    "# conf.set(\"spark.jars\", \"/Users/yixiangzhang/Desktop/postgresql-42.4.1.jar\")\n",
    "# conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark = SparkSession.builder\\\n",
    "                    .config(conf = conf)\\\n",
    "                    .appName('test').getOrCreate()\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.120.139:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1360c3950>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./silver_data/DIAGNOSES_ICD.parquet',\n",
       " './silver_data/CHARTEVENTS.parquet',\n",
       " './silver_data/D_LABITEMS.parquet',\n",
       " './silver_data/PROCEDUREEVENTS_MV.parquet',\n",
       " './silver_data/ADMISSIONS.parquet',\n",
       " './silver_data/TRANSFERS.parquet',\n",
       " './silver_data/DATETIMEEVENTS.parquet',\n",
       " './silver_data/PATIENTS.parquet',\n",
       " './silver_data/PROCEDURES_ICD.parquet',\n",
       " './silver_data/SERVICES.parquet',\n",
       " './silver_data/INPUTEVENTS_CV.parquet',\n",
       " './silver_data/MICROBIOLOGYEVENTS.parquet',\n",
       " './silver_data/D_ITEMS.parquet',\n",
       " './silver_data/LABEVENTS.parquet',\n",
       " './silver_data/DRGCODES.parquet',\n",
       " './silver_data/INPUTEVENTS_MV.parquet',\n",
       " './silver_data/ICUSTAYS.parquet',\n",
       " './silver_data/PRESCRIPTIONS.parquet',\n",
       " './silver_data/D_ICD_PROCEDURES.parquet',\n",
       " './silver_data/OUTPUTEVENTS.parquet',\n",
       " './silver_data/D_ICD_DIAGNOSES.parquet']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = \"./silver_data/\"\n",
    "filepathes = []\n",
    "for filename in os.listdir(directory):\n",
    "    if filename[-4:] == \".csv\": continue\n",
    "    \n",
    "    filepathes.append(os.path.join(directory,filename))\n",
    "\n",
    "filepathes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PRESCRIPTIONS'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.splitext(os.path.basename(filepathes[-4]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappers_pandas_types_to_spark_types = {\n",
    "    \"int\": IntegerType(),\n",
    "    \"string\": StringType(),\n",
    "    \"datetime\": TimestampType(),\n",
    "    \"float\": DoubleType()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../basic_filtered_data/mimic-iii-demo/DIAGNOSES_ICD.csv\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(filepathes[0])\n",
    "\n",
    "mimic3_path = df[\"mimic3_filepath\"].iloc[0]\n",
    "mimic3_selected = df[\"column_mimic3\"].to_list()\n",
    "mimic4_path = df[\"mimic4_filepath\"].iloc[0]\n",
    "mimic4_selected = df[\"column_mimic4_candidate_1\"].to_list()\n",
    "dtypes = df[\"dtype_mimic3\"].to_list()\n",
    "\n",
    "print(mimic3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+-------+-------+---------+\n",
      "|_c0|row_id|subject_id|hadm_id|seq_num|icd9_code|\n",
      "+---+------+----------+-------+-------+---------+\n",
      "|  0|112344|     10006| 142345|      1|    99591|\n",
      "|  1|112345|     10006| 142345|      2|    99662|\n",
      "|  2|112346|     10006| 142345|      3|     5672|\n",
      "|  3|112347|     10006| 142345|      4|    40391|\n",
      "|  4|112348|     10006| 142345|      5|    42731|\n",
      "|  5|112349|     10006| 142345|      6|     4280|\n",
      "|  6|112350|     10006| 142345|      7|     4241|\n",
      "|  7|112351|     10006| 142345|      8|     4240|\n",
      "|  8|112352|     10006| 142345|      9|     2874|\n",
      "|  9|112353|     10006| 142345|     10|    03819|\n",
      "| 10|112354|     10006| 142345|     11|     7850|\n",
      "| 11|112355|     10006| 142345|     12|    E8791|\n",
      "| 12|112356|     10006| 142345|     13|     V090|\n",
      "| 13|112357|     10006| 142345|     14|    56211|\n",
      "| 14|112358|     10006| 142345|     15|    28529|\n",
      "| 15|112359|     10006| 142345|     16|    25000|\n",
      "| 16|112360|     10006| 142345|     17|    V5867|\n",
      "| 17|112361|     10006| 142345|     18|    E9342|\n",
      "| 18|112362|     10006| 142345|     19|    41401|\n",
      "| 19|112363|     10006| 142345|     20|     2749|\n",
      "+---+------+----------+-------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/01 09:29:35 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , row_id, subject_id, hadm_id, seq_num, icd9_code\n",
      " Schema: _c0, row_id, subject_id, hadm_id, seq_num, icd9_code\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/yixiangzhang/Documents/AWS_SAA_C03/basic_filtered_data/mimic-iii-demo/DIAGNOSES_ICD.csv\n"
     ]
    }
   ],
   "source": [
    "df_m3 = spark.read.csv(mimic3_path, \n",
    "                       header=True,\n",
    "                       inferSchema=False)\n",
    "df_m3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+--------+\n",
      "|subject_id|hadm_id|seq_num|icd_code|\n",
      "+----------+-------+-------+--------+\n",
      "|     10006| 142345|      1|   99591|\n",
      "|     10006| 142345|      2|   99662|\n",
      "|     10006| 142345|      3|    5672|\n",
      "|     10006| 142345|      4|   40391|\n",
      "|     10006| 142345|      5|   42731|\n",
      "|     10006| 142345|      6|    4280|\n",
      "|     10006| 142345|      7|    4241|\n",
      "|     10006| 142345|      8|    4240|\n",
      "|     10006| 142345|      9|    2874|\n",
      "|     10006| 142345|     10|   03819|\n",
      "|     10006| 142345|     11|    7850|\n",
      "|     10006| 142345|     12|   E8791|\n",
      "|     10006| 142345|     13|    V090|\n",
      "|     10006| 142345|     14|   56211|\n",
      "|     10006| 142345|     15|   28529|\n",
      "|     10006| 142345|     16|   25000|\n",
      "|     10006| 142345|     17|   V5867|\n",
      "|     10006| 142345|     18|   E9342|\n",
      "|     10006| 142345|     19|   41401|\n",
      "|     10006| 142345|     20|    2749|\n",
      "+----------+-------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_m3 = spark_helpers.rename_columns(data = df_m3,\n",
    "                        from_cols=mimic3_selected,\n",
    "                        to_cols=mimic4_selected)\n",
    "\n",
    "df_m3_selected = df_m3.select(mimic4_selected)\n",
    "\n",
    "\n",
    "df_m3_selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructField('subject_id', IntegerType(), True)\n",
      "StructField('hadm_id', IntegerType(), True)\n",
      "StructField('seq_num', IntegerType(), True)\n",
      "StructField('icd_code', StringType(), True)\n"
     ]
    }
   ],
   "source": [
    "transfers_schema = spark_helpers.create_schema(pandas_types =df[\"dtype_mimic3\"].to_list(),\n",
    "                                                mappers = mappers_pandas_types_to_spark_types,\n",
    "                                                column_names=mimic4_selected)\n",
    "\n",
    "for item in transfers_schema:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- subject_id: integer (nullable = true)\n",
      " |-- hadm_id: integer (nullable = true)\n",
      " |-- seq_num: integer (nullable = true)\n",
      " |-- icd_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_m3_casted = spark_helpers.cast_schema(df = df_m3_selected,\n",
    "                           schema = transfers_schema\n",
    "                           )\n",
    "df_m3_casted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- subject_id: integer (nullable = true)\n",
      " |-- hadm_id: integer (nullable = true)\n",
      " |-- seq_num: integer (nullable = true)\n",
      " |-- icd_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_m4 = spark.read.csv(mimic4_path, \n",
    "                       header=True,\n",
    "                       inferSchema=False)\n",
    "\n",
    "df_m4_selected = df_m4.select(mimic4_selected)\n",
    "df_m4_casted = spark_helpers.cast_schema(df = df_m4_selected,\n",
    "                           schema = transfers_schema\n",
    "                           )\n",
    "df_m4_casted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfer_merged = df_m4_casted.coalesce(1).union(df_m3_casted.coalesce(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_helpers.write_to_db(data_frame=df_transfer_merged,\n",
    "                        table_name=os.path.splitext(os.path.basename(filepathes[-4]))[0],\n",
    "                        db_name=\"mimic\",\n",
    "                        db_usrname=\"mimic\",\n",
    "                        db_pssword=\"mimic\",\n",
    "                        port=6432)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
